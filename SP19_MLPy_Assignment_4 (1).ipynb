{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (74, 4) (74, 1) (75, 4) (75, 1)\n",
      "Base Class : Iris-virginica\n",
      "base Line : 50.66666666666667\n",
      "Accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "# Assignment_4_Question_1\n",
    "\n",
    "import pandas\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "data_raw = pandas.read_csv(\"C:\\\\Users\\\\sures\\\\Downloads\\\\iris.csv\")\n",
    "\n",
    "data = data_raw.sample(frac=1) # randomizing the data\n",
    "\n",
    "#print(data)\n",
    "\n",
    "def Z_ScoreNormalization(data,targetColumn): # we need to normalize the data so that each attribute is brought to same scale\n",
    "\n",
    "    for i in data.columns:\n",
    "\n",
    "        if i==targetColumn: # do not modify target values\n",
    "\n",
    "            continue\n",
    "\n",
    "        mean=data[i].mean()\n",
    "\n",
    "        std=data[i].std()\n",
    "        \n",
    "        data[i]=data[i].apply(lambda d : float(d-mean)/float(std)) #perform z-score normalization\n",
    "        \n",
    "def dataFilter(data,targetColumn): # filter the data and split it into two parts test and train\n",
    "\n",
    "    columnName= [col  for col in data.columns if col not in targetColumn] # get all the predictor variable names leaving target column name\n",
    "\n",
    "    dataFrame= data[columnName]# get all the data i.e X\n",
    "\n",
    "    labelFrame=data[[targetColumn]] # get all the target or labels i.e Y\n",
    "\n",
    "    size=len(dataFrame)\n",
    "\n",
    "    trainingData= dataFrame.loc[range(1,int(size/2))] # Take first half as training\n",
    "\n",
    "    trainingLabel=labelFrame.loc[range(1,int(size/2))] #take first half as training label\n",
    "\n",
    "    testData=dataFrame.loc[range(int(size/2),size)] # take second half as test\n",
    "\n",
    "    testLabel=labelFrame.loc[range(int(size/2),size)]\n",
    "\n",
    "    print (\"Shapes:\", trainingData.shape,trainingLabel.shape,testData.shape,testLabel.shape)\n",
    "\n",
    "    return trainingData,np.asarray(trainingLabel).flatten(),testData,np.asarray(testLabel).flatten() #convert all the labels into one dimensional ndarray. This is done to prevent warnings getting generated by sklearn\n",
    "\n",
    "\n",
    "def calBaseLine(data): # calculate baseline : it is percentage of records belonging to majority class\n",
    "\n",
    "    classValues=np.unique(data) # from target values find out unique classes\n",
    "\n",
    "    highest=0\n",
    "\n",
    "    baseClass=\"\"\n",
    "\n",
    "    for label in classValues: # iterate over these classes to find number of records belonging to that class\n",
    "\n",
    "        count=len(data[data==label])\n",
    "\n",
    "        if count>highest:\n",
    "\n",
    "            highest=count\n",
    "\n",
    "            baseClass=label\n",
    "\n",
    "    print  (\"Base Class :\",baseClass)\n",
    "\n",
    "    print (\"base Line :\",(float(highest)/len(data))*100)\n",
    "\n",
    "\n",
    "def calAccuracy(testLabel,predictLabel): # accuracy is percentage of correct prediction made by classifier\n",
    "    \n",
    "    count=0\n",
    "\n",
    "    for i in range(len(testLabel)):\n",
    "\n",
    "            if testLabel[i] in predictLabel[i]:\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "    print (\"Accuracy :\", (float(count) / len(testLabel))*100)\n",
    "\n",
    "\n",
    "Z_ScoreNormalization(data,\"class\") # normalize the data\n",
    "\n",
    "  \n",
    "\n",
    "training,label,test,testLabel=dataFilter(data,\"class\") # filter the data to get test and training\n",
    "\n",
    "  \n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=0.5,n_jobs=-1, solver='lbfgs', multi_class = 'auto') # create the model C=1.09 is inverse of penalization like we will see in ridge regression\n",
    "\n",
    "# it is a penalty for overfitting the model\n",
    "\n",
    "#n_jobs=-1 Number of CPU cores used during the cross-validation loop. If given a value of -1, all cores are used\n",
    "\n",
    "\n",
    "calBaseLine(testLabel)# calculate the base line\n",
    "\n",
    "  \n",
    "logreg.fit(training, label) # train our model on training data\n",
    "\n",
    "predictedLabel=logreg.predict(test)# test our model pn test data .\n",
    "\n",
    "\n",
    "calAccuracy('Iris-setosa', 'Iris-setosa')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment_4_Question_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 features based on RFE model is :  ['LDA_04' 'global_rate_negative_words' 'avg_positive_polarity'\n",
      " 'min_positive_polarity' 'shares']\n",
      "The top features based on RFECV model is :  ['LDA_01' 'LDA_02' 'LDA_04' 'global_rate_negative_words'\n",
      " 'avg_positive_polarity' 'min_positive_polarity' 'shares']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dataFrame = pandas.read_csv(\"C:\\\\Users\\\\sures\\\\Downloads\\\\OnlineNewsPopularity.csv\")\n",
    "#print(dataFrame.head())\n",
    "\n",
    "labelFrame = dataFrame['shares']\n",
    "\n",
    "model=LinearRegression(fit_intercept=True,normalize=True,n_jobs=-1)\n",
    "\n",
    "rfe = RFE(model, 5) # first parameter is the model and second parameter is number of variable to be selected\n",
    "\n",
    "rfecv = RFECV(estimator=model, step=1, cv=10) #This Will automatically select best number of features\n",
    "\n",
    "rfe = rfe.fit(dataFrame, labelFrame)#perform variable selection\n",
    "\n",
    "result=rfecv.fit(dataFrame, labelFrame)\n",
    "\n",
    "#print((rfe.ranking_)) #print ranking of importance of variables where 1 being most importance\n",
    "\n",
    "#print((result.ranking_))\n",
    "\n",
    "print (\"The top 5 features based on RFE model is : \", np.asarray(dataFrame.columns[rfe.support_]).flatten()) # print all the selected variables name. support_  is an array containing True or False value corresponding to each feature\n",
    "print (\"The top features based on RFECV model is : \",np.asarray(dataFrame.columns[result.support_]).flatten()) # flatten changes n-d array into 1-d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
