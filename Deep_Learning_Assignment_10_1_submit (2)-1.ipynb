{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code has been adopted from the course video. Text file used for training is:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/345/pg345.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# settings\n",
    "window_size = 3\n",
    "text_file = \"C:\\\\Users\\\\sures\\\\Downloads\\\\pg345.txt\"\n",
    "\n",
    "\n",
    "# functions\n",
    "def preprocessor_word(word):\n",
    "    word = word.lower().strip()\n",
    "    for punc in string.punctuation:\n",
    "        word = word.replace(punc,\"\")\n",
    "    return(word)\n",
    "\n",
    "# main\n",
    "\n",
    "# Initialize\n",
    "word_training_pairs = [] # (target, context) pairs\n",
    "\n",
    "# load text\n",
    "all_sentences = open(text_file).readlines()\n",
    "\n",
    "# extract context\n",
    "\n",
    "for sentence in all_sentences:\n",
    "    sentence_splitted = [preprocessor_word(word) for word in sentence.split()]\n",
    "    for i, target in enumerate(sentence_splitted):\n",
    "        for j in range(1,4):\n",
    "            if not i + j >= len(sentence_splitted):\n",
    "                word_training_pairs.append((target, sentence_splitted[i+j]))\n",
    "            if not i - j < 0:\n",
    "                word_training_pairs.append((target, sentence_splitted[i-j]))\n",
    "\n",
    "\n",
    "#print(word_training_pairs)\n",
    "\n",
    "id2word = list(set([pair[0] for pair in word_training_pairs]))\n",
    "#print(id2word)\n",
    "word2id = {w:i for i, w in enumerate (id2word)}\n",
    "word_training_pairs = [(word2id[pair[0]], word2id[pair[1]]) for pair in word_training_pairs  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10831"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph\n",
    "vocabulary_size = len(id2word)\n",
    "embedding_size = 50\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# variables\n",
    "\n",
    "w_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size]))\n",
    "softmax_weights =  tf.Variable(tf.random_uniform([embedding_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input\n",
    "\n",
    "train_pairs = tf.placeholder(tf.int32, shape = [None,2])\n",
    "\n",
    "\n",
    "train_input = train_pairs[:,0]\n",
    "train_output = train_pairs[:,1]\n",
    "\n",
    "# model\n",
    "\n",
    "w_embd = tf.nn.embedding_lookup(w_embeddings,train_input)\n",
    "prediction = tf.matmul(w_embd, softmax_weights)\n",
    "\n",
    "# loss\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(train_output, prediction)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_hist =[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(0, len(word_training_pairs), batch_size):\n",
    "                batch_data = word_training_pairs[i:i+batch_size]\n",
    "                #print(len(batch_data))\n",
    "                _, loss_value = sess.run([optimizer, loss], feed_dict = {train_pairs:batch_data})\n",
    "                loss_hist.append(loss_hist)\n",
    "                   \n",
    "    W = sess.run(w_embeddings)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10831, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"W.npy\", W)\n",
    "#W = np.load(\"W.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"W_pg345.npy\", W)\n",
    "#W = np.load(\"W.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Assignment_DL.txt\",\"w\")\n",
    "for w in id2word:\n",
    "    f.write(w+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%less Assignment_DL.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
