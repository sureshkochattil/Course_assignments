{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size : 3240 \n",
      "Test Size : 1081\n",
      "Accuracy : 71.69287696577243\n",
      "Base Class : positive\n",
      "base Line : 68.91766882516188\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "  \n",
    "\n",
    "relativePath=os.getcwd()\n",
    "\n",
    "rawFilePath=relativePath+\"\\Reviews.txt\" #file containing raw data\n",
    "\n",
    "processedFilePath=relativePath+\"\\ProcessedReviews.csv\" #file containing processed data\n",
    "\n",
    "  \n",
    "\n",
    "def preProcessFile(): # function to preprocess the data\n",
    "\n",
    "    file=open(rawFilePath)\n",
    "\n",
    "    writeFile=open(processedFilePath,\"w\")\n",
    "\n",
    "    badChar=\"[,!.?#@=\\n]\" #list of bad characters to be removed \n",
    "\n",
    "    for line in file:\n",
    "\n",
    "        line=line.lower().replace(\"\\t\",\" \")# First convert each word to lower case , then replace all tab space with single back space\n",
    "\n",
    "        line=re.sub(badChar,\"\",line) # using regular expression remove all bad characters\n",
    "\n",
    "        arr=line.split(\" \")# split the line using space and put all the words into a list\n",
    "\n",
    "        label=arr[0]#the first word of the list is class label i.e. either Positive or Negative\n",
    "\n",
    "        words= \" \".join(word for word in arr[1:len(arr)]) # rest of the words in the list are joined back to form the original sentence\n",
    "\n",
    "        toWrite=label+\",\"+words # line to be written: class label\n",
    "\n",
    "        writeFile.write(toWrite)\n",
    "\n",
    "        writeFile.write(\"\\n\")#after writing every line put new line character.\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    writeFile.close()\n",
    "\n",
    "\n",
    "def getDataAndLabel():\n",
    "\n",
    "    file = open(processedFilePath)# read the processed file\n",
    "\n",
    "    label=[]\n",
    "\n",
    "    data=[]\n",
    "\n",
    "    for line in file:\n",
    "\n",
    "        arr=line.replace(\"\\n\",\"\").split(\",\") #split with comma\n",
    "\n",
    "        label.append(arr[0])#first element is class label\n",
    "\n",
    "        data.append(arr[1].replace(\"\\n\",\"\"))#second element is SMS\n",
    "\n",
    "    return data,label\n",
    "\n",
    "\n",
    "def calBaseLine(data): # calculate baseline : it is percentage of records belonging to majority class\n",
    "\n",
    "    classValues=np.unique(data) # from target values find out unique classes\n",
    "\n",
    "    highest=0\n",
    "\n",
    "    baseClass=\"\"\n",
    "\n",
    "    for label in classValues: # iterate over these classes to find number of records belonging to that class\n",
    "\n",
    "        count=[i for i in data if i==label ] # create a list containing only label either ham or spam\n",
    "\n",
    "        count=len(count) #find how many of them are  ham or spam\n",
    "\n",
    "        if count>highest:\n",
    "\n",
    "            highest=count\n",
    "\n",
    "            baseClass=label\n",
    "\n",
    "    print  (\"Base Class :\",baseClass)\n",
    "\n",
    "    print (\"base Line :\",(float(highest)/len(data))*100)\n",
    "\n",
    "preProcessFile() #process the file\n",
    "\n",
    "data,label=getDataAndLabel() #get the data and label\n",
    "\n",
    "dataTrain, dataTest, labelTrain, labelTest = train_test_split( data, label, test_size=0.25, random_state=45) #split the data and label into training set and test set . 2/3 is for training and 1/3 for testing\n",
    "\n",
    "print (\"Train size :\",len(dataTrain),\"\\nTest Size :\",len(dataTest))\n",
    "\n",
    "count_vect = CountVectorizer() # instance of count vectorize\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(dataTrain) # create a numerical feature vector\n",
    "\n",
    "tfidf_transformer = TfidfTransformer() # calculate term frequency\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) #calculate Term Frequency times Inverse Document Frequency\n",
    "\n",
    "model=MultinomialNB(fit_prior=True) # create an instance of multinomial Naive Bayes\n",
    "\n",
    "model.fit(X_train_tfidf,labelTrain)# train the model\n",
    "\n",
    "X_new_counts = count_vect.transform(dataTest)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)#create Term Frequency times Inverse Document Frequency for test data\n",
    "\n",
    " \n",
    "predLabel = model.predict(X_new_tfidf)#predict the test data by using TFID\n",
    "\n",
    "#print(predLabel)\n",
    "#print(labelTest)\n",
    "\n",
    "print (\"Accuracy :\",np.mean(predLabel==labelTest)*100) #calculte accuracy\n",
    "\n",
    "calBaseLine(labelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
