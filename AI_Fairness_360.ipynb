{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment : Fairness \n",
    "## Objective\n",
    "In this assignment, we are examining the steps followed for implementing the fairness identification and mitigation software available in the AI Fairness 360 library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will install the qif360 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aif360\n",
      "  Downloading https://files.pythonhosted.org/packages/99/54/09e0674fc1370072385d64e0282eff0857e3d78c3abd7d6471200cf7a00d/aif360-0.2.2-py2.py3-none-any.whl (56.4MB)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\sures\\new_folder\\lib\\site-packages (from aif360) (1.16.3)\n",
      "Requirement already satisfied: pandas>=0.23.3 in c:\\users\\sures\\new_folder\\lib\\site-packages (from aif360) (0.24.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sures\\new_folder\\lib\\site-packages (from aif360) (0.21.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sures\\new_folder\\lib\\site-packages (from aif360) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\sures\\new_folder\\lib\\site-packages (from pandas>=0.23.3->aif360) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\sures\\new_folder\\lib\\site-packages (from pandas>=0.23.3->aif360) (2019.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sures\\new_folder\\lib\\site-packages (from scikit-learn->aif360) (0.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sures\\new_folder\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.23.3->aif360) (1.12.0)\n",
      "Installing collected packages: aif360\n",
      "Successfully installed aif360-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# Install aif360\n",
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, we will import the required dataset and Classes from the aif360 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Classes\n",
    "import numpy as np\n",
    "\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age based identification and mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will define the attribute used for identifyng the priveleged class. In this instance, we hypothesise loan applicants who are 25 and older to be enjoying a distinct privelege for availing loans, with younger applicants being biased against. Concurrently, we will drop the gender and personal status features to remove the influence of these on the bias calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set criteria for priveleged class\n",
    "dataset_orig = GermanDataset(\n",
    "\n",
    "    protected_attribute_names=['age'],                           \n",
    "\n",
    "    privileged_classes=[lambda x: x >= 25],     \n",
    "\n",
    "    features_to_drop=['personal_status', 'sex'] \n",
    "\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of training and testing, the dataset is split on a 70:30 basis, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define two features based on the age criteria we have set earlier and assign variables for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will call the BinaryLabelDatasetMetric class and use the mean_differene method on the same to calculate the mean difference in classifying applicants from the two groups we have identified based on age, as low/no risk and high risk for defaulting on their payments. We notice that there is a 13% higher probability for a favorable classification of applicants older than or equal to 25, indicating a clear bias in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.130527\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now re-weight the features to remove the influence of the bias in age, using the Reweighing class. We will then transform the original dataset by using the fit_transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the efficacy of the ai360 alrorithm, we will check for bias in the transformed dataset. As in evident, the bias relating to age has been completely eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "\n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "\n",
    "                                               privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender based identification and mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will do a similar identification and mitigation of bias based on gender, with the hypothesis that females are biased against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_sex = GermanDataset(\n",
    "\n",
    "    protected_attribute_names=['sex'],                           \n",
    "\n",
    "    privileged_classes=[lambda x: x == 'male'],     \n",
    "\n",
    "    features_to_drop=['personal_status', 'age'] \n",
    "\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train_sex, dataset_orig_test_sex = dataset_orig_sex.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will define two features based on the gender criteria and assign variables for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "\n",
    "unprivileged_groups = [{'sex': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surely enough, we notice that there is a 5% higher probability for a favorable classification of male applicants indicating a clear bias in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.055217\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train_sex = BinaryLabelDatasetMetric(dataset_orig_train_sex, \n",
    "\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train_sex.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now re-weight the features to remove the influence of the bias in age, using the Reweighing class. We will then transform the original dataset by using the fit_transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "dataset_transf_train_sex = RW.fit_transform(dataset_orig_train_sex)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On checking the bias metric, we now notice that the bias relating to gender has been completely eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train_sex = BinaryLabelDatasetMetric(dataset_transf_train_sex, \n",
    "\n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "\n",
    "                                               privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train_sex.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module of the course provided a revelatory insight into the influence of bias in machine learning / data science analysis. The bias could be at the level of the individual or groups. Bias can creep into the process either by way of a bias in the training data or in the algorithm or both. In order to identify and mitigate the biases, AI Fairness 360 provides a set of tools that are very effective and user friendly. \n",
    "While evolving a bias mitiagtion strategy, the data scientists / data stewards need to decide on the objectives - whether the goal is to remove bias at the individual or group levels. Also, considerable thought has to be given to the various options available for mitigation - pre-processing, in-process or post-processing, depending on the stages in the workflow.\n",
    "The aif360 tool is extremly versatile and simple to deploy. Its methods are intuitive and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
